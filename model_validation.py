#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯å¤šæ¨¡æ€å‡æ–°é—»æ£€æµ‹ç³»ç»Ÿçš„å„ä¸ªç»„ä»¶
"""

import os
import sys
import torch
import pandas as pd
from PIL import Image
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
root_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(root_dir)

def test_environment():
    """
    æµ‹è¯•ç¯å¢ƒé…ç½®
    """
    print("=" * 60)
    print("ç¯å¢ƒé…ç½®éªŒè¯")
    print("=" * 60)
    
    # Pythonç‰ˆæœ¬
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    # PyTorchç‰ˆæœ¬å’ŒCUDA
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.get_device_name(0)}")
    
    print("âœ“ ç¯å¢ƒé…ç½®éªŒè¯å®Œæˆ")

def test_data_loading():
    """
    æµ‹è¯•æ•°æ®åŠ è½½
    """
    print("\n" + "=" * 60)
    print("æ•°æ®åŠ è½½éªŒè¯")
    print("=" * 60)
    
    data_dir = '/root/autodl-tmp/data'
    
    # æµ‹è¯•CSVæ–‡ä»¶åŠ è½½
    try:
        train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))
        val_df = pd.read_csv(os.path.join(data_dir, 'val.csv'))
        test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))
        
        print(f"âœ“ è®­ç»ƒé›†: {len(train_df)} æ¡è®°å½•")
        print(f"âœ“ éªŒè¯é›†: {len(val_df)} æ¡è®°å½•")
        print(f"âœ“ æµ‹è¯•é›†: {len(test_df)} æ¡è®°å½•")
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        required_columns = ['path', 'text', 'label']
        for df_name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:
            missing_cols = [col for col in required_columns if col not in df.columns]
            if missing_cols:
                print(f"âœ— {df_name}ç¼ºå°‘åˆ—: {missing_cols}")
            else:
                print(f"âœ“ {df_name}æ•°æ®æ ¼å¼æ­£ç¡®")
        
        # æµ‹è¯•å›¾åƒåŠ è½½
        images_dir = os.path.join(data_dir, 'images')
        sample_path = train_df['path'].iloc[0]
        if sample_path.startswith('./data/images/'):
            image_name = sample_path.replace('./data/images/', '')
        else:
            image_name = os.path.basename(sample_path)
        
        image_path = os.path.join(images_dir, image_name)
        if os.path.exists(image_path):
            try:
                img = Image.open(image_path)
                print(f"âœ“ å›¾åƒåŠ è½½æˆåŠŸ: {img.size}")
            except Exception as e:
                print(f"âœ— å›¾åƒåŠ è½½å¤±è´¥: {e}")
        else:
            print(f"âœ— å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            
    except Exception as e:
        print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False
    
    print("âœ“ æ•°æ®åŠ è½½éªŒè¯å®Œæˆ")
    return True

def test_model_loading():
    """
    æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
    """
    print("\n" + "=" * 60)
    print("é¢„è®­ç»ƒæ¨¡å‹éªŒè¯")
    print("=" * 60)
    
    try:
        from transformers import BertModel, BertTokenizer, CLIPModel, CLIPProcessor
        
        model_cache_dir = '/root/autodl-tmp/model_cache_new'
        
        # æµ‹è¯•BERTæ¨¡å‹
        bert_path = os.path.join(model_cache_dir, 'bert-base-chinese')
        if os.path.exists(bert_path):
            bert_model = BertModel.from_pretrained(bert_path)
            bert_tokenizer = BertTokenizer.from_pretrained(bert_path)
            print(f"âœ“ BERTæ¨¡å‹åŠ è½½æˆåŠŸ: hidden_size={bert_model.config.hidden_size}")
            
            # æµ‹è¯•BERTç¼–ç 
            test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
            inputs = bert_tokenizer(test_text, return_tensors='pt', padding=True, truncation=True)
            with torch.no_grad():
                outputs = bert_model(**inputs)
            print(f"âœ“ BERTç¼–ç æµ‹è¯•æˆåŠŸ: {outputs.last_hidden_state.shape}")
        else:
            print(f"âœ— BERTæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {bert_path}")
        
        # æµ‹è¯•CLIPæ¨¡å‹
        clip_path = os.path.join(model_cache_dir, 'clip-vit-base-patch32')
        if os.path.exists(clip_path):
            clip_model = CLIPModel.from_pretrained(clip_path)
            clip_processor = CLIPProcessor.from_pretrained(clip_path)
            print(f"âœ“ CLIPæ¨¡å‹åŠ è½½æˆåŠŸ: vision_hidden_size={clip_model.config.vision_config.hidden_size}")
            
            # æµ‹è¯•CLIPç¼–ç 
            test_image = Image.new('RGB', (224, 224), color='red')
            inputs = clip_processor(images=test_image, return_tensors='pt')
            with torch.no_grad():
                image_features = clip_model.get_image_features(**inputs)
            print(f"âœ“ CLIPç¼–ç æµ‹è¯•æˆåŠŸ: {image_features.shape}")
        else:
            print(f"âœ— CLIPæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {clip_path}")
            
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False
    
    print("âœ“ é¢„è®­ç»ƒæ¨¡å‹éªŒè¯å®Œæˆ")
    return True

def test_model_components():
    """
    æµ‹è¯•æ¨¡å‹ç»„ä»¶
    """
    print("\n" + "=" * 60)
    print("æ¨¡å‹ç»„ä»¶éªŒè¯")
    print("=" * 60)
    
    try:
        # æµ‹è¯•å¯¼å…¥
        from src.models.MultiModalFakeNewsDetector import FakeNewsDataPreprocessor
        from src.models.TransformerFusionModel import create_fusion_model
        
        print("âœ“ æ¨¡å‹ç»„ä»¶å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•FakeNewsDataPreprocessoråˆå§‹åŒ–
        preprocessor = FakeNewsDataPreprocessor()
        print("âœ“ FakeNewsDataPreprocessoråˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•èåˆæ¨¡å‹åˆ›å»º - éœ€è¦å…ˆåˆ›å»ºæ–‡æœ¬å’Œå›¾åƒæ¨¡å‹
        from src.models.text_models.TextProcessingModel import TextProcessingModel
        from src.models.image_models.ImageProcessingModel import ImageProcessingModel
        
        # åˆ›å»ºç®€å•çš„æ–‡æœ¬å’Œå›¾åƒæ¨¡å‹ç”¨äºæµ‹è¯•
        text_model = TextProcessingModel(num_classes=2)
        image_model = ImageProcessingModel(num_classes=2)
        
        fusion_model = create_fusion_model(
            text_model=text_model,
            image_model=image_model,
            fusion_dim=512,
            num_classes=2
        )
        print(f"âœ“ èåˆæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
        batch_size = 2
        # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ®
        bert_input_ids = torch.randint(0, 1000, (batch_size, 128))
        bert_attention_mask = torch.ones(batch_size, 128)
        clip_input_ids = torch.randint(0, 1000, (batch_size, 77))
        clip_attention_mask = torch.ones(batch_size, 77)
        resnet_image = torch.randn(batch_size, 3, 224, 224)
        clip_pixel_values = torch.randn(batch_size, 3, 224, 224)
        
        with torch.no_grad():
            outputs = fusion_model(bert_input_ids, bert_attention_mask, clip_input_ids, clip_attention_mask, resnet_image, clip_pixel_values)
        print(f"âœ“ æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ: {outputs[0].shape}")
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹ç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("âœ“ æ¨¡å‹ç»„ä»¶éªŒè¯å®Œæˆ")
    return True

def main():
    """
    ä¸»éªŒè¯å‡½æ•°
    """
    print("å¤šæ¨¡æ€å‡æ–°é—»æ£€æµ‹ç³»ç»ŸéªŒè¯")
    print("=" * 80)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("ç¯å¢ƒé…ç½®", test_environment),
        ("æ•°æ®åŠ è½½", test_data_loading),
        ("é¢„è®­ç»ƒæ¨¡å‹", test_model_loading),
        ("æ¨¡å‹ç»„ä»¶", test_model_components)
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            result = test_func()
            results[test_name] = result if result is not None else True
        except Exception as e:
            print(f"\nâœ— {test_name}æµ‹è¯•å¼‚å¸¸: {e}")
            results[test_name] = False
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 80)
    print("éªŒè¯ç»“æœæ€»ç»“")
    print("=" * 80)
    
    all_passed = True
    for test_name, result in results.items():
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")
        if not result:
            all_passed = False
    
    print("\n" + "=" * 80)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰éªŒè¯æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå‡†å¤‡å°±ç»ªã€‚")
        print("\nä¸‹ä¸€æ­¥å¯ä»¥å¼€å§‹æ¨¡å‹è®­ç»ƒ:")
        print("cd /root/models/src/models")
        print("python train_fusion_model.py --data_dir /root/autodl-tmp/data --model_cache_dir /root/autodl-tmp/model_cache_new")
    else:
        print("âŒ éƒ¨åˆ†éªŒè¯æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®ã€‚")
    print("=" * 80)

if __name__ == '__main__':
    main()